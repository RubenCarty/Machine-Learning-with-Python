{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubenMcCarty/Machine-Learning-with-Python/blob/master/4_Me%CC%81tricas_clasificacio%CC%81n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygxPJER8zUyp"
      },
      "source": [
        "# Métricas para Clasificación Binaria\n",
        "## [M.Sc. Ruben Quispe](https://www.linkedin.com/in/ruben-quispe-l/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6XG5KnAzUyp"
      },
      "source": [
        "En clases anteriores hemos visto como podemos implementar modelos lineales para clasficación binaria, ejemplos de los modelos más simples en el `Machine Learning` pero no por ello menos usados (pese a las limitaciones que ya hemos comentado, estos modelos son eficientes y explicables, lo cual es preferido en multitud de aplicaciones). En este clase vamos a ver algunas de las métricas más utilizadas en clasificación binaria (aunque pueden extenderse también a clasificación en varias clases). Estas métricas nos van a ser muy útiles para caracterizar el desempeño de nuestros modelos y comparar diferentes modelos entre sí para, por ejemplos, utilizar el mejor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZlQTzaazUyq"
      },
      "source": [
        "## El dataset MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUDl4oK8zUyq"
      },
      "source": [
        "En las clases anteriores sobre clasificación binaria hemos trabajado con el dataset Iris para clasificación de flores a partir de características como el ancho o la longitud de sus pétalos. En este post vamos a utilizar un nuevo dataset, probablemente el más utilizado mientras aprendemos sobre `Deep Learning` y considerado el *hello world* del `Machine Learning`: el dataset [MNIST](http://yann.lecun.com/exdb/mnist/). Este dataset fue elaborado con el objetivo de entrenar modelos automáticos de OCR (*Optical Character Recognition*) para poder acelerar el trabajo de clasificación de correo a partir del código postal escrito. Está formado por 70,0000 imágenes de dígitos manuscritos, entre $0$ y $9$, y el objetivo es el de ser capaces de diseñar un algoritmo capaz de clasificar una imágen (decir que número es) directamente a partir de sus pixeles. De entre las diferentes fuentes que podemos utilizar para descargar estos datos, vamos a utilizar [Scikit-Learn](https://scikit-learn.org/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:30.332724Z",
          "start_time": "2020-07-14T11:34:11.690776Z"
        },
        "id": "Cbv_Gf5TzUyr",
        "outputId": "daa92e1a-6d94-4f83-ed3c-c90091d162b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "mnist.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:30.348758Z",
          "start_time": "2020-07-14T11:34:30.333725Z"
        },
        "id": "KWX7asI_zUys",
        "outputId": "eabb6788-ffbb-419c-df18-e15a78a347e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((70000, 784), (70000,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHwY8YEyzUys"
      },
      "source": [
        "Como puedes ver, cada imágen está formada por $784$ características (los pixeles). Podemos visualizar unas cuantas imágenes junto a sus etiquetas de la siguiente manera para hacernos una idea del tipo de datos al que nos enfrentamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:30.906759Z",
          "start_time": "2020-07-14T11:34:30.349757Z"
        },
        "id": "B-ze4oZEzUyt",
        "outputId": "1367901b-f2bf-4097-fbc6-0e1838d6a63f"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "63745",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 63745",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-824e99cbb58c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_c\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 63745"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import random \n",
        "\n",
        "r, c = 3, 5\n",
        "fig = plt.figure(figsize=(2*c, 2*r))\n",
        "for _r in range(r):\n",
        "    for _c in range(c):\n",
        "        plt.subplot(r, c, _r*c + _c + 1)\n",
        "        ix = random.randint(0, len(X)-1)\n",
        "        img = X[ix]\n",
        "        plt.imshow(img.reshape(28,28), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(y[ix])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvasSJ5-k9CS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZyFKjepzUyu"
      },
      "source": [
        "Por último, vamos a extraer $60,000$ imágenes para entrenar nuestro modelo y $10,000$ para evaluarlo y calcular las diferentes métricas que veremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.016284Z",
          "start_time": "2020-07-14T11:34:30.907761Z"
        },
        "id": "H1yTBFXYzUyu"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL-01vbzUyu"
      },
      "source": [
        "> 💡 Recuerda que para que nuestro modelo pueda aprender mejor tenemos que normalizar los datos que usaremos como entrada. En este caso cada píxel está representado por un valor entero entre 0-255, por lo que al dividir cada imágen por 255 nos aseguramos que nuestros valores a la entrada estén en el rango 0-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T09:09:07.216330Z",
          "start_time": "2020-07-14T09:09:07.203298Z"
        },
        "id": "XRNLzUr5zUyu"
      },
      "source": [
        "## Entrenando un clasificador binario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ebPOEJhzUyv"
      },
      "source": [
        "Para llevar a cabo una tarea de clasificación binaria necesitamos que nuestro dataset esté dividido en dos clases, sin embargo el dataset MNIST, cómo ya hemos visto, tiene $10$ clases (todos los dígitos entre $0$ y $9$). En este caso vamos entrenar un modelo para clasificar las imágenes con el número $5$ del resto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.032283Z",
          "start_time": "2020-07-14T11:34:31.017285Z"
        },
        "id": "cF3peFb6zUyv",
        "outputId": "99600896-6a77-4b33-c1bc-59c21c047de2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-1e29965138a2>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_train_5 = (y_train == '5').astype(np.int)\n",
            "<ipython-input-9-1e29965138a2>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_test_5 = (y_test == '5').astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_train_5 = (y_train == '5').astype(np.int)\n",
        "y_test_5 = (y_test == '5').astype(np.int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWTC9-qSzUyv"
      },
      "source": [
        "Y ahora traemos el modelo de Regresión Logística que implementamos en el [post](https://sensioai.com/blog/015_logistic_regression) anterior. Si no estás familiarizado con este modelo te recomiendo que le eches un vistazo al post antes de seguir. En este caso hemos cambiado el esquema de inicialización de los pesos, utilizando una distribución normal escalada por el número de pesos. También hemos añadido un `print` para ver cómo va evolucionando el modelo durante el entrenamiento, visualizando el valor de la *loss function* en cada *epoch*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.047282Z",
          "start_time": "2020-07-14T11:34:31.033284Z"
        },
        "code_folding": [],
        "id": "5_OoM3_gzUyv"
      },
      "outputs": [],
      "source": [
        "def bce(y, y_hat):\n",
        "    return - np.mean(y*np.log(y_hat) - (1 - y)*np.log(1 - y_hat))\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Perceptron():\n",
        "  def __init__(self, size):\n",
        "    self.w = np.random.normal(loc=0.0, \n",
        "          scale = np.sqrt(2/(size+1)), \n",
        "          size = (size, )) \n",
        "    self.ws = []\n",
        "    self.activation = sigmoid\n",
        "    self.loss = bce\n",
        "    \n",
        "  def __call__(self, w, x):\n",
        "    return self.activation(np.dot(x, w)) \n",
        "\n",
        "  def fit(self, x, y, epochs, lr, verbose=True):\n",
        "    x = np.c_[np.ones(len(x)), x]\n",
        "    for epoch in range(1,epochs+1):\n",
        "        # Batch Gradient Descent\n",
        "        y_hat = self(self.w, x)  \n",
        "        # función de pérdida\n",
        "        l = self.loss(y, y_hat)\n",
        "        # derivadas\n",
        "        dldh = (y_hat - y)\n",
        "        dhdw = x\n",
        "        dldw = np.dot(dldh, dhdw)\n",
        "        # actualizar pesos\n",
        "        self.w = self.w - lr*dldw\n",
        "        # guardar pesos para animación\n",
        "        self.ws.append(self.w.copy())\n",
        "        # print loss\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch}/{epochs} Loss {l}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.706966Z",
          "start_time": "2020-07-14T11:34:31.049283Z"
        },
        "id": "Lm_JgNITzUyw",
        "outputId": "5c850ab4-b6cc-41a1-c736-28602541117c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Loss -0.5548410142855111\n",
            "Epoch 2/10 Loss 0.7206598224991337\n",
            "Epoch 3/10 Loss 0.5304482677090383\n",
            "Epoch 4/10 Loss 0.33874121387580547\n",
            "Epoch 5/10 Loss 0.1481787367177453\n",
            "Epoch 6/10 Loss 0.029990518845371003\n",
            "Epoch 7/10 Loss 0.07018573941348813\n",
            "Epoch 8/10 Loss 0.03957115693418078\n",
            "Epoch 9/10 Loss 0.05379890044402431\n",
            "Epoch 10/10 Loss 0.041989565540281604\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "perceptron = Perceptron(X.shape[1] + 1)\n",
        "epochs, lr = 10, 1e-5\n",
        "perceptron.fit(X_train, y_train_5, epochs, lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PPYFSezUyw"
      },
      "source": [
        "En este punto ya hemos entrenado nuestro modelo. Ahora, ¿cómo lo podemos evaluar? ¿Es este un buen modelo o un mal modelo? ¿Si entrenamos otro modelo, cómo podemos saber si es mejor o pero que el primero? Todas estas preguntas las podemos responder con las `métricas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6gN1w_DzUyw"
      },
      "source": [
        "## Métricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYY4PN7ZzUyw"
      },
      "source": [
        "Vamos a ver las métricas más utilizadas en la evaluación de modelos de clasificación binaria. Para calcular cualquier métrica vamos a necesitar ser capaces de generar predicciones con nuestro modelo. En posts anterior ya hemos visto cómo podemos hacerlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.722963Z",
          "start_time": "2020-07-14T11:34:31.707968Z"
        },
        "id": "1wlh3JiUzUyw"
      },
      "outputs": [],
      "source": [
        "def evaluate(perceptron, x, t = 0.5):\n",
        "    w = perceptron.ws[-1]\n",
        "    x = np.c_[np.ones(len(x)), x]\n",
        "    y = perceptron(w, x)\n",
        "    return (y > t).astype(np.int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3gDBixIzUyw"
      },
      "source": [
        "### *Accuracy*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScCn1A3ZzUyw"
      },
      "source": [
        "La primera métrica que vamos a ver, también la más común, es la *accuracy* (o precisión). En esta métrica simplemente contamos todos los elementos del dataset que nuestro modelo ha sido capaz de clasificar correctamente y lo dividimos entre el número total de elementos. Ésto implica que la precisión será un valor entre $0$ y $1$, significando $0$ que nuestro modelo no ha acertado ningún resultado y $1$ que los ha acertado todos. También es común dar este valor en porcentaje (simplemente multiplicando por $100$ el resultado anterior)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.738962Z",
          "start_time": "2020-07-14T11:34:31.723965Z"
        },
        "id": "a9Ib-WHPzUyx"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y):\n",
        "    return np.sum(y_pred == y) / len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.880965Z",
          "start_time": "2020-07-14T11:34:31.739962Z"
        },
        "id": "hiCpe2xEzUyx",
        "outputId": "24c767a3-2cba-47da-906a-1e6f6fe7916c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-7df743ea669f>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return (y > t).astype(np.int)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9196666666666666"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = evaluate(perceptron, X_train)\n",
        "accuracy(y_pred, y_train_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.912963Z",
          "start_time": "2020-07-14T11:34:31.881965Z"
        },
        "id": "_s_9wzBazUyx",
        "outputId": "fffe9d21-839a-4ae0-a87f-fe6826908699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-7df743ea669f>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return (y > t).astype(np.int)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9232"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = evaluate(perceptron, X_test)\n",
        "accuracy(y_pred, y_test_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNdG3q0PzUyx"
      },
      "source": [
        "Cómo podemos ver nuestro modelo tiene una precisión cercana al $92 \\%$ tanto para el conjunto de entrenamiento como para el de evaluación. ¿Es este un buen modelo? A priori podríamos pensar que si, ya que nuestro modelo acierta $9$ de cada $10$ imágenes... Sin embargo, la realidad es que nuestro modelo no está haciendo prácticamente nada. Vamos a ver porqué analizando nuestros datos. Cómo ya hemos comentado estamos haciendo un clasificador binario para detectar el número $5$. ¿Cuántas imágenes diferentes de $5$s tenemos en nuestro dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:31.928962Z",
          "start_time": "2020-07-14T11:34:31.913963Z"
        },
        "id": "IxWJg-UKzUyx",
        "outputId": "fe83fdb8-0769-45b1-b3c0-746185ad89ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.90965, 0.9108)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1 - y_train_5.mean(), 1 - y_test_5.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmNviOFozUyx"
      },
      "source": [
        "Aquí tenemos la respuesta, nuestro dataset está formado en un $91 \\%$ por imágenes que no son $5$. Esto significa que un modelo *naive* que siempre diga que NO tenemos un $5$ va a tener una *accuracy* del $91 \\%$. Sin embargo, en el mundo real, nunca detectaremos este dígito y nuestra aplicación fallará estrepitosamente. Nuestro modelo tiene una precisión del $92 \\%$, mejorando ligreamente este valor pero no por mucho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:32.447968Z",
          "start_time": "2020-07-14T11:34:31.929963Z"
        },
        "id": "1EVu9KwgzUyx"
      },
      "outputs": [],
      "source": [
        "r, c = 3, 5\n",
        "fig = plt.figure(figsize=(2*c, 2*r))\n",
        "for _r in range(r):\n",
        "    for _c in range(c):\n",
        "        plt.subplot(r, c, _r*c + _c + 1)\n",
        "        fives = (y_test_5 == 1)\n",
        "        ix = random.randint(0, len(X_test[fives])-1)\n",
        "        img = X_test[fives][ix]\n",
        "        plt.imshow(img.reshape(28,28), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "        y_pred = evaluate(perceptron, [img])\n",
        "        plt.title(f\"{y_test_5[fives][ix]} / {y_pred[0]}\", color=\"red\" if y_test_5[fives][ix] != y_pred[0] else \"green\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVOnm-8WzUyx"
      },
      "source": [
        "Como puedes ver la métrica de precisión es muy útil pero puede llevar a engaños. Esto se hace especialmente patente en aplicaciones en las que tenemos muy pocas muestras de la clase que queremos capturar. Un ejemplo claro es el de detección de transacciones bancarias fraudulentas, que representan cerca del 0.001% del total de operaciones realizadas. Un modelo de clasificación binaria que siempre diese como resultado que una transacción NO es fraudulenta tendría una precisión del 99.999%, sin embargo sería un modelo totalmente inútil en el mundo real."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMlQHRBWzUyx"
      },
      "source": [
        "> ⚡ Siempre es recomendable tener unas primeras métricas `baseline` con las que comparar nuestros modelos. Estas métricas pueden obtenerse con un modelo aleatorio (antes de ser entrenado) o en el caso de la clasificación podemos utilizar directamente la distribución de clases en el dataset. Cualquier modelo que hagamos debería mejorar estas métricas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR6OIrAnzUyx"
      },
      "source": [
        "En este caso en particular podemos obtener un mejor modelo simplemente entrenando durante más epochs (el modelo anterior no había convergido)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:48.937986Z",
          "start_time": "2020-07-14T11:34:32.448966Z"
        },
        "id": "oNC3TZp9zUyx"
      },
      "outputs": [],
      "source": [
        "perceptron = Perceptron(X.shape[1] + 1)\n",
        "epochs, lr = 300, 1e-5\n",
        "perceptron.fit(X_train, y_train_5, epochs, lr, verbose=False)\n",
        "y_pred = evaluate(perceptron, X_test)\n",
        "accuracy(y_pred, y_test_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.525005Z",
          "start_time": "2020-07-14T11:34:48.938989Z"
        },
        "id": "7q0avm2GzUyy"
      },
      "outputs": [],
      "source": [
        "r, c = 3, 5\n",
        "fig = plt.figure(figsize=(2*c, 2*r))\n",
        "for _r in range(r):\n",
        "    for _c in range(c):\n",
        "        plt.subplot(r, c, _r*c + _c + 1)\n",
        "        fives = (y_test_5 == 1)\n",
        "        ix = random.randint(0, len(X_test[fives])-1)\n",
        "        img = X_test[fives][ix]\n",
        "        plt.imshow(img.reshape(28,28), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "        y_preds = evaluate(perceptron, [img])\n",
        "        plt.title(f\"{y_test_5[fives][ix]} / {y_preds[0]}\", color=\"red\" if y_test_5[fives][ix] != y_preds[0] else \"green\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mucX0O8YzUyy"
      },
      "source": [
        "Todas las métricas que vamos a ver están implementadas en la librería `Scikit Learn`, por lo que te recomiendo que las uses directamente para evitar errores en la implementación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.555999Z",
          "start_time": "2020-07-14T11:34:49.526005Z"
        },
        "id": "46EUhKkyzUyy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_test_5, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zu1-BzszUyy"
      },
      "source": [
        "### La Matriz de Confusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShGzmOK9zUyy"
      },
      "source": [
        "Una matriz de confusión nos va a indicar exactamente cuales son los puntos fuertes y débiles de nuestro clasificador. Para cada clase, vamos a calcular cuantos elementos han sido bien clasificados por nuestro modelo y cuántos han sido confundidos con otras clases (esta matriz nos será muy útil en clasificación en varias clases también)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.571999Z",
          "start_time": "2020-07-14T11:34:49.556999Z"
        },
        "id": "HxN14FWwzUyy"
      },
      "outputs": [],
      "source": [
        "TP = np.sum((y_pred == 1) & (y_test_5 == 1)) \n",
        "TN = np.sum((y_pred == 0) & (y_test_5 == 0)) \n",
        "FP = np.sum((y_pred == 1) & (y_test_5 == 0))\n",
        "FN = np.sum((y_pred == 0) & (y_test_5 == 1))\n",
        "\n",
        "CM = [[TN, FP],\n",
        "      [FN, TP]]\n",
        "CM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V92brzDzUyy"
      },
      "source": [
        "En esta matriz podemos observar:\n",
        "    \n",
        "- Verdaderos Positivos (TP, *True Positives*): elementos que nuestro modelo clasifica correctamente como la clase que nos interesa, la clase positiva (fila 1, columna 1).\n",
        "- Verdaderos Negativos (TN, *True Negatives*): elementos que nuestro modelo clasifica correctamente como la clase negativa (fila 0, columna 0).\n",
        "- Falsos Positivos (FP, *False Positives*): elementos que nuestro modelo clasifica erróneamente como la clase positiva (fila 0, columna 1).\n",
        "- Falsos Negativos (FN, *False Negatives*): elementos que nuestro modelo clasifica erróneamente como la clase negativa (fila 1, columna 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.587999Z",
          "start_time": "2020-07-14T11:34:49.572999Z"
        },
        "id": "YnINkm4LzUyy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test_5, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QopQe1OgzUyy"
      },
      "source": [
        "> 💡 Una vez vistos los conceptos definidos en este apartado podemos reescribir la *accuracy* como $ accuracy = \\frac{TP + TN}{TP+TN+FP+FN} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcHTgviJzUyy"
      },
      "source": [
        "### *Precision* y *Recall*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rprLcS_szUyz"
      },
      "source": [
        "Hemos visto la confusión a la que nos puede llevar una métrica simple como la *accuracy*. Ahora veremos dos métricas que un poco más informativas. Éstas son *precision* (no confundir con la precisión, en castellano, equivalente a la *accuracy*) y *recall*. Para definir estas métricas primero necesitamos definir los siguiente conceptos:\n",
        "\n",
        "$$ precision = \\frac{TP}{TP+FP} $$\n",
        "\n",
        "$$ recall = \\frac{TP}{TP+FN} $$\n",
        "\n",
        "En la siguiente imágen puedes ver una visualización de estas métricas. *Precision* nos da una idea de cómo de propenso es nuestro modelo a dar falsos positivos. Un valor cercano a $1$ indicará que nuestro modelo apenas da falsos positivos (aunque puede seguir fallando dando falsos negativos) mientras que un valor cercano a $0$ indicará que nuestro modelo da muchos falsos positivos. Lo mismo aplica al *Recall*, pero en este caso aplicada a los falsos negativos. Un valor cercano a $1$ indicará que nuestro modelo apenas da falsos negativos (aunque puede seguir fallando dando falsos positivos) mientras que un valor cercano a $0$ indicará que nuestro modelo da muchos falsos negativos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T10:17:58.102236Z",
          "start_time": "2020-07-14T10:17:58.050344Z"
        },
        "id": "Xj194QFQzUyz"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/800px-Precisionrecall.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.604000Z",
          "start_time": "2020-07-14T11:34:49.588999Z"
        },
        "id": "oYQbChldzUyz"
      },
      "outputs": [],
      "source": [
        "def precision(y_pred, y):\n",
        "    TP = np.sum((y_pred == 1) & (y == 1)) \n",
        "    FP = np.sum((y_pred == 1) & (y == 0))\n",
        "    return TP / (TP + FP)\n",
        "\n",
        "def recall(y_pred, y):\n",
        "    TP = np.sum((y_pred == 1) & (y == 1)) \n",
        "    FN = np.sum((y_pred == 0) & (y == 1))\n",
        "    return TP / (TP + FN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.636000Z",
          "start_time": "2020-07-14T11:34:49.605003Z"
        },
        "id": "LMm1ix52zUyz"
      },
      "outputs": [],
      "source": [
        "y_pred = evaluate(perceptron, X_test)\n",
        "precision(y_pred, y_test_5), recall(y_pred, y_test_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:49.652002Z",
          "start_time": "2020-07-14T11:34:49.637999Z"
        },
        "id": "CFiPUbltzUyz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision_score(y_test_5, y_pred), recall_score(y_test_5, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtbYyvShzUyz"
      },
      "source": [
        "En función de la aplicación en la que estemos trabajando vamos a querer optimizar una métrica u otra. En aplicaciones en las que tener falsos positivos sea perjudicial (por ejemplo, aplicaciones de seguridad) querremos modelos con un buena *precision*, mientras que en aplicaciones en las que tener falsos negativos sea perjudicial (por ejemplo, sistemas de diagnóstico médico) querremos modelos con buen *recall*. Una vez nuestro modelo ha sido entrenado, podemos ajustar estas métricas variando el valor del *threshold* utilizado en la evaluación. A esto se le conoce como el *precision-recall trade off*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.082002Z",
          "start_time": "2020-07-14T11:34:49.654000Z"
        },
        "id": "6pPkVhXmzUyz"
      },
      "outputs": [],
      "source": [
        "for t in np.linspace(0.1,0.9,20):\n",
        "    y_pred = evaluate(perceptron, X_test, t)\n",
        "    print(f\"Threshold: {t:.3f} Precision {precision(y_pred, y_test_5):.4f} Recall {recall(y_pred, y_test_5):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPnHJFb1zUyz"
      },
      "source": [
        "Cómo se puede observar, incrementar el valor de una de las métricas implica que la otra va a disminuir (es por esto que se le llama *trade-off* ya que tenemos que llegar a un compromiso). Así pues, escogeremos un valor del *threshold* que se ajusto a los criterios de diseño de nuestra aplicación en particular (en función de la cantidad de falsos positivos y falsos negativos que estemos dispuestos a asumir)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T10:45:15.188117Z",
          "start_time": "2020-07-14T10:45:15.144035Z"
        },
        "id": "0RjK-rRPzUyz"
      },
      "source": [
        "![](https://wizardforcel.gitbooks.io/scikit-and-tensorflow-workbooks-bjpcjp/pics/decision-threshold-and-precision-vs-recall.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7aXfZOlzUyz"
      },
      "source": [
        "### F1-Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckmUYCdBzUyz"
      },
      "source": [
        "Una métrica rápida que nos va a decir si nuestro modelo tiene buena *precision* y *recall* es el *F1-score*.\n",
        "\n",
        "$$  F_1 = 2 \\times \\frac{precision \\times recall}{precision + recall} $$\n",
        "\n",
        "Es una métrica muy usada ya que aglutina la información de ambas métricas, sin embargo va a favorecer mucho aquellos modelos con un valor similar de *precision* y *recall*, lo cual ya hemos comentado que no es siempre lo que vamos a querer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.114041Z",
          "start_time": "2020-07-14T11:34:50.083004Z"
        },
        "id": "QFfwMuxqzUyz"
      },
      "outputs": [],
      "source": [
        "y_pred = evaluate(perceptron, X_test)\n",
        "\n",
        "p = precision_score(y_test_5, y_pred)\n",
        "r = recall_score(y_test_5, y_pred)\n",
        "\n",
        "f1 = 2*(p*r)/(p+r)\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.130040Z",
          "start_time": "2020-07-14T11:34:50.115041Z"
        },
        "id": "U7Vdj-8AzUyz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_test_5, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocZ9E5DzUyz"
      },
      "source": [
        "### La curva ROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAx-VjFJzUy0"
      },
      "source": [
        "Podemos visualizar rápidamente el comportamiento de un modelo para varios *thresholds* con la llamada curva ROC (*Receiver Operating Characteristic*). En ella representamos el ratio de verdaderos positivos (TPR) contra el ratio de falsos positivos (FPR), definidos de la siguiente manera:\n",
        "\n",
        "$$ TPR = \\frac{TP}{TP+FN} $$\n",
        "\n",
        "$$ FPR = \\frac{FP}{FP+TN} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.145040Z",
          "start_time": "2020-07-14T11:34:50.131040Z"
        },
        "id": "VSkDtbpwzUy0"
      },
      "outputs": [],
      "source": [
        "def evaluate2(perceptron, x):\n",
        "    w = perceptron.ws[-1]\n",
        "    x = np.c_[np.ones(len(x)), x]\n",
        "    y = perceptron(w, x)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.176040Z",
          "start_time": "2020-07-14T11:34:50.146040Z"
        },
        "id": "8FEnL3wDzUy0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "y_pred2 = evaluate2(perceptron, X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test_5, y_pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.272041Z",
          "start_time": "2020-07-14T11:34:50.177040Z"
        },
        "id": "uj0_sVQ4zUy0"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(fpr, tpr, label=None):\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
        "    plt.plot([0, 1], [0, 1], 'k--') \n",
        "    plt.axis([0, 1, 0, 1])                                    \n",
        "    plt.xlabel('FPR', fontsize=16) \n",
        "    plt.ylabel('TPR', fontsize=16)    \n",
        "    plt.grid(True)                                            \n",
        "\n",
        "plt.figure(figsize=(8, 6))                         \n",
        "plot_roc_curve(fpr, tpr)              \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTzUjDGIzUy0"
      },
      "source": [
        "Una métrica muy utilizada para comparar clasificadores es el área bajo la curva ROC, ya que nos indica como de robusto es un modelo. La línea recta representa un modelo *naive* que podemos usar como baseline. Cómo puedes ver el área bajo la curva ROC de este modelo es de $0.5$, por lo que cualquier modelo que hagamos debería superar este valor. Cuánto más se acerque la curva a la esquina superior izquierda, mayor será el área y mejor será el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-14T11:34:50.288041Z",
          "start_time": "2020-07-14T11:34:50.273041Z"
        },
        "id": "NfmbUDd1zUy0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_score(y_test_5, y_pred2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh4FxbQnzUy0"
      },
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kO7kAMXzUy0"
      },
      "source": [
        "En este post hemos visto las métricas más comunes cuando trabajamos con clasificadores (tanto binarios como multi-clase). Para ello hemos trabajado con el dataset MNIST, uno de los datasets más utilizados en el `Machine Learning`. El objetivo es el de clasificar imágenes de dígitos manuscritos en 10 clases (desde el dígito 0 al 9), sin embargo lo hemos adaptado para hacer clasificación binaria de imágenes con el número 5. También hemos mejorado ligeramente la implementación de nuestro `Perceptrón` con una mejor inicialización de los pesos y la posibilidad de visualizar la evolución de la función de pérdida durante el entrenamiento. Después hemos visto las siguiente métricas:\n",
        "\n",
        "- *Accuracy* es la más simple, consistente en contar cuantos elementos del dataset nuestro modelo clasifica correctamente. Puede llevarnos a engaños si no tenemos en cuenta la distribución de nuestras clases, poniendo en relevancia la necesidad de tener métricas `baseline` para comprar nuestros primeros modelos.\n",
        "- La matriz de confusión nos va indicar el número de elementos del nuestro dataset que el modelo clasifica bien, así como todos los que confunda con otras clases. De esta manera sabremos cómo mejorar nuestro modelo.\n",
        "- *Precision* y *Recall* nos indican cómo de robusto será nuestro modelo, cuantificando los falsos positivos y falsos negativos. En función de nuestra aplicación y su sensibilidad preferiremos una mejor métrica u otra (como hemos visto, mejorar una de ella implica empeorar la otra).\n",
        "- El *F1-Score* nos dará una idea rápida de la robustez del modelo combinando las dos métricas anteriores.\n",
        "- La curva *ROC*, y en especial el área debajo de la curva, será una muy buena manera de comparar modelos para poder elegir el que más se ajuste a nuestras necesidades.\n",
        "\n",
        "Como podemos ver, evaluar un modelo va más allá de proporcionar un valor de precisión y dependerá en gran medida de la aplicación en cuestión y las condiciones reales a las que se vaya a enfrentar nuestro modelo en el mundo real. Monitorizar nuestros modelos constantemente nos permitirá mejorarlos de manera contínua así como encontrar posibles fallos, y para ello saber qué métricas son las importantes es fundamental."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfOkdIm6zUy0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}